# day211115 - Precision and Recall in Depth

[https://en.wikipedia.org/wiki/Precision_and_recall](https://en.wikipedia.org/wiki/Precision_and_recall)

Precision can be seen as a **measure of quality**, and recall as a **measure of quantity**. Higher precision means that an algorithm returns more relevant results than irrelevant ones, and high recall means that an algorithm returns most of the relevant results (whether or not irrelevant ones are also returned).

## Introduction

In [information retrieval](https://en.wikipedia.org/wiki/Information_retrieval), the instances are documents and **the task is to return a set of relevant documents** given a search term. Recall is the *number of relevant documents* retrieved by a search *divided by the total number of existing relevant documents*, while precision is the *number of relevant documents* retrieved by a search *divided by the total number of documents retrieved* by that search.

In a [classification](https://en.wikipedia.org/wiki/Classification_(machine_learning)) task, the precision for a class is the *number of true positives* (i.e. the number of items correctly labelled as belonging to the positive class) *divided by the total number of elements labelled as belonging to the positive class* (i.e. the sum of true positives and [false positives](https://en.wikipedia.org/wiki/Type_I_and_type_II_errors), which are items incorrectly labelled as belonging to the class). Recall in this context is defined as the *number of true positives divided by the total number of elements that actually belong to the positive class* (i.e. the sum of true positives and [false negatives](https://en.wikipedia.org/wiki/Type_I_and_type_II_errors), which are items which were not labelled as belonging to the positive class but should have been).

In information retrieval, a perfect precision score of 1.0 means that every result retrieved by a search was relevant (but says nothing about whether all relevant documents were retrieved) whereas a perfect recall score of 1.0 means that all relevant documents were retrieved by the search (but says nothing about how many irrelevant documents were also retrieved).

Precision and recall are not particularly useful metrics when used in isolation. For instance, it is possible to have perfect recall by simply retrieving every single item. Likewise, it is possible to have near-perfect precision by selecting only a very small number of extremely likely items.

In a classification task, a precision score of 1.0 for a class C means that every item labelled as belonging to class C does indeed belong to class C (but says nothing about the number of items from class C that were not labelled correctly) whereas a recall of 1.0 means that every item from class C was labelled as belonging to class C (but says nothing about how many items from other classes were incorrectly also labelled as belonging to class C).

Often, there is an inverse relationship between precision and recall, where it is possible to increase one at the cost of reducing the other. Brain surgery provides an illustrative example of the tradeoff. Consider a brain surgeon removing a cancerous tumor from a patient’s brain. The surgeon needs to remove all of the tumour cells since any remaining cancer cells will regenerate the tumor. Conversely, the surgeon must not remove healthy brain cells since that would leave the patient with impaired brain function. The surgeon may be more liberal in the area of the brain he removes to ensure he has extracted all the cancer cells. This decision increases recall but reduces precision. On the other hand, the surgeon may be more conservative in the brain he removes to ensure he extracts only cancer cells. This decision increases precision but reduces recall. That is to say, greater recall increases the chances of removing healthy cells (negative outcome) and increases the chances of removing all cancer cells (positive outcome). Greater precision decreases the chances of removing healthy cells (positive outcome) but also decreases the chances of removing all cancer cells (negative outcome).

Usually, precision and recall scores are not discussed in isolation. Instead, either values for one measure are compared for a fixed level at the other measure (e.g. *precision at a recall level of 0.75*) or both are combined into a single measure. Examples of measures that are a combination of precision and recall are the [F-measure](https://en.wikipedia.org/wiki/Precision_and_recall#F-measure) (the weighted [harmonic mean](https://en.wikipedia.org/wiki/Harmonic_mean) of precision and recall), or the [Matthews correlation coefficient](https://en.wikipedia.org/wiki/Matthews_correlation_coefficient), which is a [geometric mean](https://en.wikipedia.org/wiki/Geometric_mean) of the chance-corrected variants: the [regression coefficients](https://en.wikipedia.org/wiki/Regression_coefficient) [Informedness](https://en.wikipedia.org/wiki/Informedness) (DeltaP') and [Markedness](https://en.wikipedia.org/wiki/Markedness) (DeltaP).[[1]](https://en.wikipedia.org/wiki/Precision_and_recall#cite_note-Powers2011-1)[[2]](https://en.wikipedia.org/wiki/Precision_and_recall#cite_note-2) [Accuracy](https://en.wikipedia.org/wiki/Accuracy_(binary_classification)) is a weighted arithmetic mean of Precision and Inverse Precision (weighted by Bias) as well as a weighted arithmetic mean of Recall and Inverse Recall (weighted by Prevalence).[[1]](https://en.wikipedia.org/wiki/Precision_and_recall#cite_note-Powers2011-1) Inverse Precision and Inverse Recall are simply the Precision and Recall of the inverse problem where positive and negative labels are exchanged (for both real classes and prediction labels). Recall and Inverse Recall, or equivalently true positive rate and false positive rate, are frequently plotted against each other as [ROC](https://en.wikipedia.org/wiki/Receiver_operating_characteristic) curves and provide a principled mechanism to explore operating point tradeoffs. Outside of Information Retrieval, the application of Recall, Precision and F-measure are argued to be flawed as they ignore the true negative cell of the contingency table, and they are easily manipulated by biasing the predictions.[[1]](https://en.wikipedia.org/wiki/Precision_and_recall#cite_note-Powers2011-1) The first problem is 'solved' by using [Accuracy](https://en.wikipedia.org/wiki/Accuracy_(binary_classification)) and the second problem is 'solved' by discounting the chance component and renormalizing to [Cohen's kappa](https://en.wikipedia.org/wiki/Cohen%27s_kappa), but this no longer affords the opportunity to explore tradeoffs graphically. However, [Informedness](https://en.wikipedia.org/wiki/Informedness) and [Markedness](https://en.wikipedia.org/wiki/Markedness) are Kappa-like renormalizations of Recall and Precision,[[3]](https://en.wikipedia.org/wiki/Precision_and_recall#cite_note-3) and their geometric mean [Matthews correlation coefficient](https://en.wikipedia.org/wiki/Matthews_correlation_coefficient) thus acts like a debiased F-measure.