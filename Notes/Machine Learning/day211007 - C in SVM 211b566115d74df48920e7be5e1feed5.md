# day211007 - C in SVM

[https://stats.stackexchange.com/questions/31066/what-is-the-influence-of-c-in-svms-with-linear-kernel](https://stats.stackexchange.com/questions/31066/what-is-the-influence-of-c-in-svms-with-linear-kernel)

For large values of C, the optimization will choose a smaller-margin hyperplane if that hyperplane does a better job of getting all the training points classified correctly. 

In a SVM you are searching for two things: a hyperplane with the largest minimum margin, and a hyperplane that correctly separates as many instances as possible. The problem is that you will not always be able to get both things. The c parameter determines how great your desire is for the latter. I have drawn a small example below to illustrate this. To the left you have a low c which gives you a pretty large minimum margin (purple). However, this requires that we neglect the blue circle outlier that we have failed to classify correct. On the right you have a high c. Now you will not neglect the outlier and thus end up with a much smaller margin.

![Untitled](day211007%20-%20C%20in%20SVM%20211b566115d74df48920e7be5e1feed5/Untitled.png)

So which of these classifiers are the best? That depends on what the future data you will predict looks like, and most often you don't know that of course. If the future data looks like this:

![Untitled](day211007%20-%20C%20in%20SVM%20211b566115d74df48920e7be5e1feed5/Untitled%201.png)

then the classifier learned using a large c value is best.

On the other hand, if the future data looks like this:

![Untitled](day211007%20-%20C%20in%20SVM%20211b566115d74df48920e7be5e1feed5/Untitled%202.png)

then the classifier learned using a low c value is best.

Depending on your data set, changing c may or may not produce a different hyperplane. If it does produce a different hyperplane, that does not imply that your classifier will output different classes for the particular data you have used it to classify. Weka is a good tool for visualizing data and playing around with different settings for an SVM. It may help you get a better idea of how your data look and why changing the c value does not change the classification error. In general, having few training instances and many attributes make it easier to make a linear separation of the data. Also that fact that you are evaluating on your training data and not new unseen data makes separation easier.

**In my words**

C is the degree for how much you want to avoid misclassifying. If it is too large, it strictly divides into classes. It has a chance to allow outliers to be classified too. On the other hand, if it is too small. it has a tendency not to consider outliers. It doesn't seem to see outliers as observations and just neglects them.

Either case has pros and cons. Sometimes it is better to use a more relaxing hyperplane for the statistics and vice versa. 

**In someone else's words**

If **c is small**, the penalty for misclassified points is low so a decision boundary with **a large margin is chosen**.

If **c is large**, SVM tries to minimize the number of misclassified examples due to high penalty which results in a decision boundary **with a smaller margin**.