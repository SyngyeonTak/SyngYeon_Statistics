# day211104 - PCA statquest

**Intro**

There happen to be many features for the dataset.

If there are more than 3 features to be considered it is really hard to draw the graph (The more dimensions, the more complicated it gets). So most of time we have to think how to reduce the number of dimensionality without the significant loss of a dataset or prediction

**To begin with**

So we could simply think of two features for the PCA. first we have to get the mean from two features. And we shift the data so that the center is on top of the origin (0, 0) in the graph

![Untitled](day211104%20-%20PCA%20statquest%201f00632097c74d0fa5f7a4b80bb4551d/Untitled.png)

And we find the best line to fit

![Untitled](day211104%20-%20PCA%20statquest%201f00632097c74d0fa5f7a4b80bb4551d/Untitled%201.png)

The main idea to find the best line is...

**Pythagorean Theorem**

![the a is the line from the origin to the point. and the value for a doesn't change. So if the line gets closer to the point and the distance from the origin to the point of line is larger.  To put it, maximizing the sum of the squared distances from the projected points to the origin is the best fitting line](day211104%20-%20PCA%20statquest%201f00632097c74d0fa5f7a4b80bb4551d/Untitled%202.png)

the a is the line from the origin to the point. and the value for a doesn't change. So if the line gets closer to the point and the distance from the origin to the point of line is larger.  To put it, maximizing the sum of the squared distances from the projected points to the origin is the best fitting line

We keep rotating the line until we end up with the line with the largest sum of squared distances between the projected points and the origin

And the best fitting line is called Principal Component 1 (PC1)

**Terminology**

**Linear Combination**: When it comes to the best fitting line... If the values spread out along axes X more than along Y, we say axes X is more important than the counterpart. 

When you do PCA with SVD, the recipe for PC1 is scaled so that this length = 1

![Untitled](day211104%20-%20PCA%20statquest%201f00632097c74d0fa5f7a4b80bb4551d/Untitled%203.png)

The rest of the values will be also scaled as per the line. 

To make PC1

Mix 0.97 parts Gene 1 with 0.242 parts Gene 2.

This 1 unit long vector, consisting of 0.97 parts Gene 1 and 0.242 parts Gene 2, is called the "**Singular Vector**" or the "**Eigenvector**" for PC1

And the proportions of each gene are called "**Loading Scores**"

**PC2**

Since this is only a 2-D graph, PC2 is simply the line through the origin that is **perpendicular to PC1**, without any further optimization that has to be done

This means that the recipe for PC2 is -1 parts Gene 1 and 4 parts Gene 2

**Drawing the PCA graph**

We simply rotate everything so that PC1 is horizontal

![Untitled](day211104%20-%20PCA%20statquest%201f00632097c74d0fa5f7a4b80bb4551d/Untitled%204.png)

![Untitled](day211104%20-%20PCA%20statquest%201f00632097c74d0fa5f7a4b80bb4551d/Untitled%205.png)

**Calculating percent variation for each PC and scree plot**

We can convert them into variation around the origin (0, 0) by dividing by the sample size minus 1 (i.e. n-1) 

![Untitled](day211104%20-%20PCA%20statquest%201f00632097c74d0fa5f7a4b80bb4551d/Untitled%206.png)

**Scree Plot**

a graphical representation of the percentages of variation that each PC accounts for

![Untitled](day211104%20-%20PCA%20statquest%201f00632097c74d0fa5f7a4b80bb4551d/Untitled%207.png)

**Now we are going into the example with 3 variables taken into a consideration**

![Untitled](day211104%20-%20PCA%20statquest%201f00632097c74d0fa5f7a4b80bb4551d/Untitled%208.png)

![Untitled](day211104%20-%20PCA%20statquest%201f00632097c74d0fa5f7a4b80bb4551d/Untitled%209.png)

**Closing**

The good thing about PCA is that it tells which variables are significant no matter how many variables to be considered. The graph would have been really messy if we put whole variables.

Note:

 If the scree plot where PC3 and PC4 account for a substantial amount of variation, then just using the first 2 PCs would not create a very accurate representation of the data

![Untitled](day211104%20-%20PCA%20statquest%201f00632097c74d0fa5f7a4b80bb4551d/Untitled%2010.png)

 

However, even a noisy PCA plot like this can be used to identify clusters of data

![Untitled](day211104%20-%20PCA%20statquest%201f00632097c74d0fa5f7a4b80bb4551d/Untitled%2011.png)