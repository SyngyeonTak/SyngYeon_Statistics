# day210919 - Decision Trees

- **Intro**

    **Here's a simple decision tree...**

    ![Untitled](day210919%20-%20Decision%20Trees%209a70b23b9d254836ae541a150dbc2756/Untitled.png)

    In general, a decision tree asks a question

    And then classifies the person based on the answer.

    **Another decision tree...**

    ![Untitled](day210919%20-%20Decision%20Trees%209a70b23b9d254836ae541a150dbc2756/Untitled%201.png)

    This decision tree is based on ranked data, where 1 is super hungry and 2 is moderately hungry

    Note: The classification can be categories or numeric

- **Example**

    **Here's a more complicated decision tree.**

    ![Untitled](day210919%20-%20Decision%20Trees%209a70b23b9d254836ae541a150dbc2756/Untitled%202.png)

    It combines numeric data, with 'yes/ no data

    And the order of questions on the left side, first about resting heart rate then about eating doughnuts...

    doesn't have to be the same on the right side. On the right side, the question about doughnuts is asked first

    lastly the final questions can be repeated

    ### **Jargons**

    **Root node**: The very top of the tree

    **Internal nodes (nodes):** have arrows pointing to them, and they have arrows pointing away from    them

    **Leaf node (Leaves)**: have arrows pointing to them, but there are no arrows pointing away from them

    ![Untitled](day210919%20-%20Decision%20Trees%209a70b23b9d254836ae541a150dbc2756/Untitled%203.png)

    ![Untitled](day210919%20-%20Decision%20Trees%209a70b23b9d254836ae541a150dbc2756/Untitled%204.png)

    **Now let's talk about how to go from a raw table of data**

    ![Untitled](day210919%20-%20Decision%20Trees%209a70b23b9d254836ae541a150dbc2756/Untitled%205.png)

    In this example, we want to create a tree that uses **chest pain, good blood circulation and blocked artery status** to predict, whether or not the patient has a heart disease

    The first thing we want to know is whether Chest Pain, Good Blood Circulation or Blocked Arteries should be at the very top of our tree

    We start by looking at how well Chest Pain alone predicts heart disease

    Here's a little tree that only takes chest pain into account

    ![Untitled](day210919%20-%20Decision%20Trees%209a70b23b9d254836ae541a150dbc2756/Untitled%206.png)

    Ultimately, we look at chest pain and heart disease for all 303 patients in this study

    ![Untitled](day210919%20-%20Decision%20Trees%209a70b23b9d254836ae541a150dbc2756/Untitled%207.png)

    Now we do the exact same thing for Good Blood Circulation

    ![Untitled](day210919%20-%20Decision%20Trees%209a70b23b9d254836ae541a150dbc2756/Untitled%208.png)

    lastly for the Blocked arteries

    Note: The total number of patients with heart disease is different for Chest Pain, Good Blood Circulation and Blocked Arteries because some patients had measurements for Chest Pain, but not for Blocked Arteries, etc.

- **Calculating Gini Impurity - 1**

    **Jargons**

    **Impure**: None of the leaf notes is 100% "Yes Heart Disease" or 100% "No Heart Disease"

    â†’ To determine which separation is best, we need a way to measure and compare "**impurity**"

    **Gini** : It is one of the ways to measure **impurity**

    **Let's start by calculating Gini impurity for Chest Pain**

    equation: $1 - \text{(the probability of yes)}^2 - \text{(the probability of no)}^2$

    ![Untitled](day210919%20-%20Decision%20Trees%209a70b23b9d254836ae541a150dbc2756/Untitled%209.png)

    ![Untitled](day210919%20-%20Decision%20Trees%209a70b23b9d254836ae541a150dbc2756/Untitled%2010.png)

    ![Untitled](day210919%20-%20Decision%20Trees%209a70b23b9d254836ae541a150dbc2756/Untitled%2011.png)

    Having Chest Pain - Heart Disease Gini impurity = 0.395

    Not Having Chest Pain - Heart Disease Gini impurity = 0.336

    Now that we have measured the Gini impurity for both leaf nodes, **we can calculate the total Gini impurity** for using Chest Pain to separate patients with and without heart disease.

    Since the leaf nodes do not represent the same number of patients, the total Gini impurity for using Chest Pain to separate patients with and without heart disease is **the weighted average of the leaf node impurities**.

    ![Untitled](day210919%20-%20Decision%20Trees%209a70b23b9d254836ae541a150dbc2756/Untitled%2012.png)

    ![Untitled](day210919%20-%20Decision%20Trees%209a70b23b9d254836ae541a150dbc2756/Untitled%2013.png)

    Good Blood Circulation has the lowest impurity (it separates patients with and without heart disease the best)

    When we divided all of the patients using Good Blood Circulation, we ended up with "impure" left nodes.

    Each leaf contained a mixture of patients with and without Heart Disease

- **Calculating Gini Impurity - 2**

    Now we need to figure how well chest pain and blocked arteries separate these 164 patients (37 with heart disease and 127 without heart disease).

    ![Untitled](day210919%20-%20Decision%20Trees%209a70b23b9d254836ae541a150dbc2756/Untitled%2014.png)

    ![Untitled](day210919%20-%20Decision%20Trees%209a70b23b9d254836ae541a150dbc2756/Untitled%2015.png)

    Since Blocked arteries has the lowest Gini impurity, we will use it at this node to separate patients

    ![Untitled](day210919%20-%20Decision%20Trees%209a70b23b9d254836ae541a150dbc2756/Untitled%2016.png)

- **Calculating Gini Impurity - 3**

    To put it...

    1) Calculate all of the Gini impurity scores

    2) If the node itself has the lowest score, than there is no point in separating the patients any more and it becomes a leaf node.

    3) If separating the data results in an improvement, than pick the separation with the lowest impurity value

    ![Untitled](day210919%20-%20Decision%20Trees%209a70b23b9d254836ae541a150dbc2756/Untitled%2017.png)

- **How do we determine what's the best weight to use to divide the patients?**

    Step 1) Sort the patients by weight, lowest to highest

    Step 2) Calculate the average weight for all adjacent patients.

    Step 3) Calculate the impurity Values for each average weight

    ![Untitled](day210919%20-%20Decision%20Trees%209a70b23b9d254836ae541a150dbc2756/Untitled%2018.png)

    For example, we can calculate the impurity value for weights less than 167.5

    Gini impurity for Weight < 167.5 is the weighted average of the impurities for the two leaves

    ![Untitled](day210919%20-%20Decision%20Trees%209a70b23b9d254836ae541a150dbc2756/Untitled%2019.png)

- **Ranked data**

    ![Untitled](day210919%20-%20Decision%20Trees%209a70b23b9d254836ae541a150dbc2756/Untitled%2020.png)

    When there are multiple choices, like **"color choice can be blue, green or red",** you calculate an impurity score for each one as well as each possible combination.

    For this example, with three colors (blue, green, and red) we get the following options

    ![Untitled](day210919%20-%20Decision%20Trees%209a70b23b9d254836ae541a150dbc2756/Untitled%2021.png)